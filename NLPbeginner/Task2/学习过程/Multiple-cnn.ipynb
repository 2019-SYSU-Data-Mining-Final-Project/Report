{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T08:40:34.426993Z",
     "start_time": "2019-05-21T08:40:20.759Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'rb')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def clean_doc(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "\tdump(dataset, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "file_train = pd.read_csv('train.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "doc = list()\n",
    "for i in file_train[\"Sentence\"]:\n",
    "    doc.append(clean_doc(i))\n",
    "xtrain = doc \n",
    "y = file_train[\"Sentiment\"]\n",
    "ytrain = []\n",
    "NumberofSize = file_train[\"Sentiment\"].size\n",
    "for i in list(range(0,NumberofSize)):\n",
    "        if file_train[\"Sentiment\"][i] == 'negative' :\n",
    "            ytrain.append(0)\n",
    "        if file_train[\"Sentiment\"][i] == 'neutral' :\n",
    "            ytrain.append(1)\n",
    "        if file_train[\"Sentiment\"][i] == 'positive' :\n",
    "            ytrain.append(2)\n",
    "ytrain = np.array(ytrain)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "ytrain = to_categorical(ytrain,3)\n",
    "save_dataset([xtrain,ytrain], 'train.pkl')\n",
    "#print(xtrain[0])\n",
    "\n",
    "\n",
    "\n",
    "file_test = pd.read_csv('test.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "doc = list()\n",
    "for i in file_test[\"Sentence\"]:\n",
    "    doc.append(clean_doc(i))\n",
    "xtest = doc \n",
    "for i in range(0,file_testr[\"Sentiment\"].size):\n",
    "    ytest.append(0)\n",
    "ytest = np.array(ytest)\n",
    "save_dataset([xtest,ytest], 'test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T07:40:19.543495Z",
     "start_time": "2019-05-21T07:30:38.358574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 42\n",
      "Vocabulary size: 23256\n",
      "(10026, 42)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 42, 100)      2325600     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 42, 100)      2325600     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 42, 100)      2325600     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 39, 32)       12832       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 37, 32)       19232       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 35, 32)       25632       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 39, 32)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 37, 32)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 35, 32)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 19, 32)       0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 18, 32)       0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 17, 32)       0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 608)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 576)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 544)          0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1728)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           17290       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            33          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,051,819\n",
      "Trainable params: 7,051,819\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8020 samples, validate on 2006 samples\n",
      "Epoch 1/10\n",
      "8020/8020 [==============================] - 58s 7ms/step - loss: 0.9657 - acc: 0.5262 - val_loss: 0.9589 - val_acc: 0.5828\n",
      "Epoch 2/10\n",
      "8020/8020 [==============================] - 57s 7ms/step - loss: 0.5631 - acc: 0.7737 - val_loss: 0.8946 - val_acc: 0.6112\n",
      "Epoch 3/10\n",
      "8020/8020 [==============================] - 58s 7ms/step - loss: 0.1435 - acc: 0.9532 - val_loss: 1.0213 - val_acc: 0.5892\n",
      "Epoch 4/10\n",
      "8020/8020 [==============================] - 57s 7ms/step - loss: 0.0434 - acc: 0.9894 - val_loss: 1.1524 - val_acc: 0.6042\n",
      "Epoch 5/10\n",
      "8020/8020 [==============================] - 58s 7ms/step - loss: 0.0232 - acc: 0.9948 - val_loss: 1.2047 - val_acc: 0.5842\n",
      "Epoch 6/10\n",
      "8020/8020 [==============================] - 57s 7ms/step - loss: 0.0174 - acc: 0.9964 - val_loss: 1.1880 - val_acc: 0.5882\n",
      "Epoch 7/10\n",
      "8020/8020 [==============================] - 59s 7ms/step - loss: 0.0127 - acc: 0.9973 - val_loss: 1.3363 - val_acc: 0.6047\n",
      "Epoch 8/10\n",
      "8020/8020 [==============================] - 59s 7ms/step - loss: 0.0110 - acc: 0.9979 - val_loss: 1.3828 - val_acc: 0.5932\n",
      "Epoch 9/10\n",
      "8020/8020 [==============================] - 57s 7ms/step - loss: 0.0097 - acc: 0.9976 - val_loss: 1.3842 - val_acc: 0.5877\n",
      "Epoch 10/10\n",
      "8020/8020 [==============================] - 57s 7ms/step - loss: 0.0077 - acc: 0.9983 - val_loss: 1.4121 - val_acc: 0.5917\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(3, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model\n",
    "\n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)\n",
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX],array(trainLabels), validation_split = 0.2 ,epochs=10, batch_size=16)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T08:41:07.917432Z",
     "start_time": "2019-05-21T08:40:49.273989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 42\n",
      "Vocabulary size: 23256\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "(10026, 42) (4850, 42)\n",
      "Train Accuracy: 91.751446\n",
      "[[5.9514374e-01 7.7429116e-03 1.5357733e-03]\n",
      " [1.4647841e-04 2.7095377e-03 4.8537850e-03]\n",
      " [1.6975552e-02 2.8869808e-01 1.4538169e-03]\n",
      " ...\n",
      " [7.2340190e-01 3.4855008e-03 2.4922192e-03]\n",
      " [2.2023916e-05 8.4182024e-03 2.1578130e-01]\n",
      " [1.2159646e-03 1.4361161e-03 2.1285225e-02]]\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n",
    "\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n",
    "\n",
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "testLines,_ = load_dataset('test.pkl')\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "print(type(trainLines[0]))\n",
    "print(type(testLines[0]))\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    "\n",
    "# evaluate model on test dataset dataset\n",
    "result = model.predict([testX,testX,testX])\n",
    "temp = np.argmax(result, axis=1)\n",
    "tran_result = []\n",
    "print(result)\n",
    "#print(shape(result))\n",
    "for i in temp:\n",
    "    #print(result[i])\n",
    "    if i == 0 :\n",
    "        tran_result.append('negative')\n",
    "        continue\n",
    "    if i == 1 :\n",
    "        tran_result.append('neutral')\n",
    "        continue\n",
    "    if i == 2:\n",
    "        tran_result.append('positive')\n",
    "        continue\n",
    "    #print(result[i])\n",
    "output = pd.DataFrame( data={\"id\":file_test[\"ID2\"],\"polarity\":tran_result} )\n",
    "output.to_csv( \"final.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
