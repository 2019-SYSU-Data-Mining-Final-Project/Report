{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:21:02.650967Z",
     "start_time": "2019-05-24T05:20:52.361032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "D:\\ANACONDA\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 引入对应的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import gensim.models.word2vec as w2v\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:21:06.177745Z",
     "start_time": "2019-05-24T05:21:06.152228Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义 TextCNN网络结构\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(textCNN,self).__init__()\n",
    "        vocb_size = args['vocb_size']\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        max_len = args['max_len']\n",
    "        \n",
    "        embedding_matrix = args['embedding_matrix']\n",
    "        # 填入预训练好的词向量\n",
    "        self.embeding = nn.Embedding(vocb_size,dim,_weight=embedding_matrix)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5,stride=1,padding=2),\n",
    "                    nn.BatchNorm2d(16),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5,stride=1,padding=2),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,stride=1,padding=2),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=5,stride=1,padding=2),\n",
    "                    nn.BatchNorm2d(128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.out = nn.Linear(1536,n_class)\n",
    "    def forward(self,x):\n",
    "        x = self.embeding(x)\n",
    "        #print('embeding: {}'.format(x))\n",
    "        x = x.view(x.size(0),1,max_len,word_dim)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1: {}'.format(x))\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2: {}'.format(x))\n",
    "        x = self.conv3(x)\n",
    "        #print('conv3: {}'.format(x))\n",
    "        x = self.conv4(x)\n",
    "        #print('conv4: {}'.format(x))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        #print('final: {}'.format(x))\n",
    "        output = self.out(x)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:21:11.154531Z",
     "start_time": "2019-05-24T05:21:10.469227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33167\n",
      "[('I', 3404), ('tomorrow', 1874), ('u', 1016), ('going', 994), ('The', 961), ('may', 932), ('night', 819), ('go', 811), ('see', 718), ('time', 717), ('day', 704), ('Sunday', 673), ('Im', 659), ('amp', 652), ('Saturday', 637), ('like', 631), ('get', 619), ('tonight', 567), ('one', 543), ('Friday', 536), ('today', 477), ('game', 465), ('got', 439), ('back', 411), ('Day', 396), ('last', 393), ('think', 383), ('good', 378), ('want', 368), ('come', 368), ('know', 368), ('Monday', 361), ('still', 358), ('new', 346), ('Thursday', 336), ('make', 323), ('us', 322), ('show', 307), ('So', 295), ('RT', 294), ('If', 284), ('next', 281), ('first', 280), ('We', 276), ('dont', 270), ('love', 268), ('wait', 264), ('watch', 254), ('meet', 252), ('morning', 251)]\n",
      "33167\n"
     ]
    }
   ],
   "source": [
    "# 生成词汇表\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "def clean_doc(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(doc,vocab):\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "file_train = pd.read_csv('C:/Users/WYX/Desktop/test/data/train.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "text_train = \"\"\n",
    "for i in file_train[\"Sentence\"]:\n",
    "    text_train += i\n",
    "    text_train += '\\n'\n",
    "\n",
    "file_test = pd.read_csv('C:/Users/WYX/Desktop/test/data/test.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "text_test = \"\"\n",
    "for i in file_test[\"Sentence\"]:\n",
    "    text_test += i\n",
    "    text_test += '\\n'\n",
    "\n",
    "\n",
    "vocab = Counter()\n",
    "add_doc_to_vocab(text_train,vocab)\n",
    "add_doc_to_vocab(text_test,vocab)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "min_occurane = 0\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "save_list(tokens, 'C:/Users/WYX/Desktop/test/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:21:25.946325Z",
     "start_time": "2019-05-24T05:21:12.747100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 14876\n",
      "Vocabulary size: 33167\n"
     ]
    }
   ],
   "source": [
    "# 生成随机word embedding\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "\n",
    "\n",
    "vocab_filename = 'C:/Users/WYX/Desktop/test/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "file_train = pd.read_csv('C:/Users/WYX/Desktop/test/data/train.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "doc = list()\n",
    "for i in file_train[\"Sentence\"]:\n",
    "    doc += (doc_to_clean_lines(i,vocab))\n",
    "\n",
    "file_test = pd.read_csv('C:/Users/WYX/Desktop/test/data/test.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "for i in file_test[\"Sentence\"]:\n",
    "    doc += (doc_to_clean_lines(i,vocab))\n",
    "sentences = doc\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "filename = 'C:/Users/WYX/Desktop/test/embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:38:40.951440Z",
     "start_time": "2019-05-24T05:38:14.481847Z"
    }
   },
   "outputs": [],
   "source": [
    "#读入测试集并对 text 进行处理\n",
    "\n",
    "# 文本处理函数\n",
    "def clean_doc(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "# 读入训练集 \n",
    "file_train = pd.read_csv('C:/Users/WYX/Desktop/test/data/train.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "doc = list()\n",
    "for i in file_train[\"Sentence\"]:\n",
    "    doc.append(clean_doc(i))\n",
    "train_docs = doc\n",
    "\n",
    "# 训练集对应标签\n",
    "y = file_train[\"Sentiment\"]\n",
    "ytrain = []\n",
    "NumberofSize = file_train[\"Sentiment\"].size\n",
    "for i in list(range(0,NumberofSize)):\n",
    "        if file_train[\"Sentiment\"][i] == 'negative' :\n",
    "            ytrain.append(0)\n",
    "        if file_train[\"Sentiment\"][i] == 'neutral' :\n",
    "            ytrain.append(1)\n",
    "        if file_train[\"Sentiment\"][i] == 'positive' :\n",
    "            ytrain.append(2)\n",
    "ytrain = np.array(ytrain)\n",
    "\n",
    "#读入测试集\n",
    "file_test = pd.read_csv('C:/Users/WYX/Desktop/test/data/test.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "doc = list()\n",
    "for i in file_test[\"Sentence\"]:\n",
    "    doc.append(clean_doc(i))\n",
    "test_docs = doc\n",
    "\n",
    "# 读入词表\n",
    "vocab_filename = 'C:/Users/WYX/Desktop/test/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "max_len = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "# 将输入的文本转换为对应的 word2vec\n",
    "tokenizer = Tokenizer()\n",
    "# 训练集\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "encoded_xtrain = tokenizer.texts_to_sequences(train_docs)\n",
    "Xtrain = pad_sequences(encoded_xtrain,maxlen=max_len,padding='post')\n",
    "# 测试集\n",
    "tokenizer.fit_on_texts(test_docs)\n",
    "encoded_xtest = tokenizer.texts_to_sequences(test_docs)\n",
    "Xtest = pad_sequences(encoded_xtest,maxlen=max_len,padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用个人训练的 embeding \n",
    "# 定义装载函数\n",
    "def load_embedding(filename):\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "# 得到 embeding 层权重\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word,0)\n",
    "        #print(weight_matrix[i])\n",
    "    return weight_matrix\n",
    "\n",
    "# 得到 embeding matrix\n",
    "raw_embedding = load_embedding('C:/Users/WYX/Desktop/test/embedding_word2vec.txt')\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用谷歌训练好的 embedding glove.300B.840.txt\n",
    "\n",
    "def load_embedding(filename):\n",
    "    file = open(filename,'rb')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word,0)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    "\n",
    "raw_embedding = load_embedding('C:/Users/WYX/Desktop/test/glove.840B.300d.txt')\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:38:40.967333Z",
     "start_time": "2019-05-24T05:38:40.956599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28777, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:38:41.040335Z",
     "start_time": "2019-05-24T05:38:40.977886Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置 textCNN参数\n",
    "vocb_size = len(tokenizer.word_index) + 1\n",
    "n_class = 3\n",
    "nb_words = vocb_size \n",
    "word_dim = 100\n",
    "args = {}\n",
    "args['vocb_size'] = vocb_size \n",
    "args['max_len'] = max_len\n",
    "args['n_class'] = n_class\n",
    "args['dim'] = 100\n",
    "args['embedding_matrix'] = torch.Tensor(embedding_vectors)\n",
    "EPOCH = 10\n",
    "test_epoch_size = 300\n",
    "epoch_size = 1000 \n",
    "LR = 0.000001\n",
    "cnn = textCNN(args)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(),lr=LR)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T05:42:31.000743Z",
     "start_time": "2019-05-24T05:39:43.379622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss is: 1.1255340576171875\n",
      "now acc is: 0.397\n",
      "now loss is: 1.1234824657440186\n",
      "now acc is: 0.391\n",
      "now loss is: 1.0989079475402832\n",
      "now acc is: 0.441\n",
      "now loss is: 1.1156374216079712\n",
      "now acc is: 0.433\n",
      "now loss is: 1.119517207145691\n",
      "now acc is: 0.393\n",
      "now loss is: 1.1422700881958008\n",
      "now acc is: 0.355\n",
      "now loss is: 1.1401242017745972\n",
      "now acc is: 0.355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b7c29ca8f4db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mb_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;31m#print('b_y:\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print(b_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c6512bcf993b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#print('conv1: {}'.format(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;31m#print('conv2: {}'.format(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 338\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i in range(0,(int)(len(Xtrain)/epoch_size)):\n",
    "        b_x = Variable(torch.LongTensor(Xtrain[i*epoch_size:i*epoch_size+epoch_size]))\n",
    "        b_y = Variable(torch.LongTensor(ytrain[i*epoch_size:i*epoch_size+epoch_size]))\n",
    "        output = cnn(b_x) + 0.0001\n",
    "        #print('b_y:\\n')\n",
    "        #print(b_y)\n",
    "        #print('output:\\n')\n",
    "        #print(output)\n",
    "        loss = loss_function(output,b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('now loss is: {}'.format(loss))\n",
    "        pred_y = torch.max(output,1)[1].data.squeeze()\n",
    "        acc = (b_y == pred_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        accuracy = acc / (b_y.size(0))\n",
    "        print('now acc is: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T03:42:22.555899Z",
     "start_time": "2019-05-23T03:42:22.546088Z"
    }
   },
   "outputs": [],
   "source": [
    "# 由于个人机器能力有限 在保证 cnn 运行正确的基础上停止了长时间运行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
