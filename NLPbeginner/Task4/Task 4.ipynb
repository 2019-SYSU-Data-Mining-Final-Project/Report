{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "- 使用 LSTM+CRF 完成序列标注任务\n",
    "- 阅读论文了解实体识别与 CRF+LSTM 做序列标注的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T12:58:52.206300Z",
     "start_time": "2019-06-01T12:58:52.201315Z"
    }
   },
   "source": [
    "## 简单实现 LSTM 序列标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:09.541135Z",
     "start_time": "2019-06-09T11:08:09.533130Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将句子转换为 word vector\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        idxs.append(to_ix[w])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:12.156213Z",
     "start_time": "2019-06-09T11:08:12.128290Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "# 准备训练集\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print (word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:13.710148Z",
     "start_time": "2019-06-09T11:08:13.167538Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 LSTM 序列标注类\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:14.456338Z",
     "start_time": "2019-06-09T11:08:14.447359Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "import torch.optim as optim\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:22.271815Z",
     "start_time": "2019-06-09T11:08:15.605640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1119968891143799\n",
      "1.1467570066452026\n",
      "1.1116259098052979\n",
      "1.146106481552124\n",
      "1.111256718635559\n",
      "1.145458459854126\n",
      "1.1108894348144531\n",
      "1.144813060760498\n",
      "1.1105239391326904\n",
      "1.1441702842712402\n",
      "1.11016047000885\n",
      "1.143530011177063\n",
      "1.109798789024353\n",
      "1.1428923606872559\n",
      "1.1094390153884888\n",
      "1.1422573328018188\n",
      "1.1090810298919678\n",
      "1.1416246891021729\n",
      "1.10872483253479\n",
      "1.1409947872161865\n",
      "1.1083705425262451\n",
      "1.1403672695159912\n",
      "1.1080178022384644\n",
      "1.139742374420166\n",
      "1.1076672077178955\n",
      "1.1391198635101318\n",
      "1.1073181629180908\n",
      "1.1384998559951782\n",
      "1.106971025466919\n",
      "1.1378822326660156\n",
      "1.1066255569458008\n",
      "1.1372672319412231\n",
      "1.1062818765640259\n",
      "1.1366546154022217\n",
      "1.1059401035308838\n",
      "1.1360445022583008\n",
      "1.1055997610092163\n",
      "1.135436773300171\n",
      "1.1052614450454712\n",
      "1.134831428527832\n",
      "1.1049246788024902\n",
      "1.1342284679412842\n",
      "1.1045897006988525\n",
      "1.1336278915405273\n",
      "1.1042563915252686\n",
      "1.1330296993255615\n",
      "1.1039247512817383\n",
      "1.1324340105056763\n",
      "1.1035947799682617\n",
      "1.131840467453003\n",
      "1.1032664775848389\n",
      "1.1312494277954102\n",
      "1.1029399633407593\n",
      "1.1306606531143188\n",
      "1.1026147603988647\n",
      "1.1300742626190186\n",
      "1.1022915840148926\n",
      "1.1294901371002197\n",
      "1.1019699573516846\n",
      "1.1289082765579224\n",
      "1.1016498804092407\n",
      "1.128328800201416\n",
      "1.1013314723968506\n",
      "1.1277514696121216\n",
      "1.1010147333145142\n",
      "1.1271765232086182\n",
      "1.1006993055343628\n",
      "1.1266037225723267\n",
      "1.1003857851028442\n",
      "1.1260331869125366\n",
      "1.1000736951828003\n",
      "1.1254650354385376\n",
      "1.0997631549835205\n",
      "1.1248990297317505\n",
      "1.0994542837142944\n",
      "1.1243350505828857\n",
      "1.099146842956543\n",
      "1.123773455619812\n",
      "1.0988409519195557\n",
      "1.1232140064239502\n",
      "1.098536729812622\n",
      "1.1226568222045898\n",
      "1.098233938217163\n",
      "1.1221017837524414\n",
      "1.0979325771331787\n",
      "1.1215487718582153\n",
      "1.0976330041885376\n",
      "1.1209979057312012\n",
      "1.097334623336792\n",
      "1.1204493045806885\n",
      "1.0970377922058105\n",
      "1.1199027299880981\n",
      "1.0967425107955933\n",
      "1.1193584203720093\n",
      "1.0964486598968506\n",
      "1.1188161373138428\n",
      "1.096156358718872\n",
      "1.1182758808135986\n",
      "1.0958654880523682\n",
      "1.1177377700805664\n",
      "1.0955759286880493\n",
      "1.117201566696167\n",
      "1.0952880382537842\n",
      "1.1166677474975586\n",
      "1.095001459121704\n",
      "1.116135835647583\n",
      "1.0947163105010986\n",
      "1.1156059503555298\n",
      "1.0944325923919678\n",
      "1.115078091621399\n",
      "1.0941503047943115\n",
      "1.1145521402359009\n",
      "1.0938692092895508\n",
      "1.1140283346176147\n",
      "1.0935896635055542\n",
      "1.1135064363479614\n",
      "1.0933115482330322\n",
      "1.1129865646362305\n",
      "1.0930347442626953\n",
      "1.1124687194824219\n",
      "1.092759370803833\n",
      "1.1119529008865356\n",
      "1.0924851894378662\n",
      "1.1114389896392822\n",
      "1.092212438583374\n",
      "1.1109269857406616\n",
      "1.091940999031067\n",
      "1.1104168891906738\n",
      "1.0916709899902344\n",
      "1.1099088191986084\n",
      "1.0914021730422974\n",
      "1.1094026565551758\n",
      "1.091134786605835\n",
      "1.108898401260376\n",
      "1.0908687114715576\n",
      "1.108396053314209\n",
      "1.0906039476394653\n",
      "1.1078956127166748\n",
      "1.0903403759002686\n",
      "1.1073968410491943\n",
      "1.0900781154632568\n",
      "1.1069002151489258\n",
      "1.0898170471191406\n",
      "1.106405258178711\n",
      "1.089557409286499\n",
      "1.105912208557129\n",
      "1.089298963546753\n",
      "1.1054210662841797\n",
      "1.0890417098999023\n",
      "1.1049318313598633\n",
      "1.0887857675552368\n",
      "1.1044442653656006\n",
      "1.088531255722046\n",
      "1.1039586067199707\n",
      "1.0882775783538818\n",
      "1.103474736213684\n",
      "1.088025450706482\n",
      "1.1029926538467407\n",
      "1.0877745151519775\n",
      "1.1025123596191406\n",
      "1.0875245332717896\n",
      "1.1020338535308838\n",
      "1.0872758626937866\n",
      "1.1015570163726807\n",
      "1.0870285034179688\n",
      "1.1010819673538208\n",
      "1.0867822170257568\n",
      "1.1006087064743042\n",
      "1.0865371227264404\n",
      "1.1001373529434204\n",
      "1.0862932205200195\n",
      "1.0996675491333008\n",
      "1.0860505104064941\n",
      "1.0991995334625244\n",
      "1.0858091115951538\n",
      "1.0987331867218018\n",
      "1.0855684280395508\n",
      "1.0982685089111328\n",
      "1.085329294204712\n",
      "1.0978056192398071\n",
      "1.085091233253479\n",
      "1.0973443984985352\n",
      "1.084854006767273\n",
      "1.096884846687317\n",
      "1.0846182107925415\n",
      "1.0964269638061523\n",
      "1.0843833684921265\n",
      "1.0959707498550415\n",
      "1.084149718284607\n",
      "1.0955160856246948\n",
      "1.083917260169983\n",
      "1.0950632095336914\n",
      "1.0836856365203857\n",
      "1.0946118831634521\n",
      "1.0834553241729736\n",
      "1.0941622257232666\n",
      "1.083225965499878\n",
      "1.0937142372131348\n",
      "1.0829975605010986\n",
      "1.093267798423767\n",
      "1.0827704668045044\n",
      "1.0928229093551636\n",
      "1.0825443267822266\n",
      "1.0923796892166138\n",
      "1.0823192596435547\n",
      "1.0919380187988281\n",
      "1.0820952653884888\n",
      "1.0914978981018066\n",
      "1.0818722248077393\n",
      "1.0910594463348389\n",
      "1.0816502571105957\n",
      "1.0906224250793457\n",
      "1.081429362297058\n",
      "1.0901870727539062\n",
      "1.0812095403671265\n",
      "1.0897531509399414\n",
      "1.0809905529022217\n",
      "1.0893208980560303\n",
      "1.0807726383209229\n",
      "1.0888900756835938\n",
      "1.0805559158325195\n",
      "1.0884606838226318\n",
      "1.080340027809143\n",
      "1.0880329608917236\n",
      "1.080125093460083\n",
      "1.0876067876815796\n",
      "1.0799111127853394\n",
      "1.0871820449829102\n",
      "1.0796982049942017\n",
      "1.0867587327957153\n",
      "1.0794861316680908\n",
      "1.0863368511199951\n",
      "1.079275131225586\n",
      "1.085916519165039\n",
      "1.0790650844573975\n",
      "1.0854976177215576\n",
      "1.0788558721542358\n",
      "1.0850801467895508\n",
      "1.0786478519439697\n",
      "1.084664225578308\n",
      "1.078440546989441\n",
      "1.0842496156692505\n",
      "1.078234314918518\n",
      "1.083836555480957\n",
      "1.078028917312622\n",
      "1.0834248065948486\n",
      "1.0778244733810425\n",
      "1.0830146074295044\n",
      "1.0776208639144897\n",
      "1.0826057195663452\n",
      "1.0774180889129639\n",
      "1.0821982622146606\n",
      "1.0772165060043335\n",
      "1.0817921161651611\n",
      "1.0770156383514404\n",
      "1.0813875198364258\n",
      "1.0768157243728638\n",
      "1.0809842348098755\n",
      "1.076616644859314\n",
      "1.0805822610855103\n",
      "1.0764185190200806\n",
      "1.08018159866333\n",
      "1.076221227645874\n",
      "1.079782485961914\n",
      "1.0760247707366943\n",
      "1.0793845653533936\n",
      "1.0758291482925415\n",
      "1.0789880752563477\n",
      "1.0756343603134155\n",
      "1.0785930156707764\n",
      "1.075440526008606\n",
      "1.0781989097595215\n",
      "1.0752475261688232\n",
      "1.0778064727783203\n",
      "1.0750552415847778\n",
      "1.077415108680725\n",
      "1.0748637914657593\n",
      "1.0770251750946045\n",
      "1.0746732950210571\n",
      "1.076636552810669\n",
      "1.0744836330413818\n",
      "1.076249122619629\n",
      "1.0742946863174438\n",
      "1.0758631229400635\n",
      "1.0741064548492432\n",
      "1.0754783153533936\n",
      "1.0739191770553589\n",
      "1.0750948190689087\n",
      "1.0737327337265015\n",
      "1.0747125148773193\n",
      "1.0735470056533813\n",
      "1.074331521987915\n",
      "1.073362112045288\n",
      "1.0739517211914062\n",
      "1.0731779336929321\n",
      "1.0735732316970825\n",
      "1.072994589805603\n",
      "1.0731959342956543\n",
      "1.0728120803833008\n",
      "1.0728198289871216\n",
      "1.0726302862167358\n",
      "1.0724449157714844\n",
      "1.0724492073059082\n",
      "1.0720713138580322\n",
      "1.0722688436508179\n",
      "1.0716989040374756\n",
      "1.072089433670044\n",
      "1.0713276863098145\n",
      "1.0719105005264282\n",
      "1.0709576606750488\n",
      "1.071732521057129\n",
      "1.0705888271331787\n",
      "1.0715551376342773\n",
      "1.070221185684204\n",
      "1.0713787078857422\n",
      "1.069854736328125\n",
      "1.0712028741836548\n",
      "1.0694893598556519\n",
      "1.0710277557373047\n",
      "1.0691251754760742\n",
      "1.0708532333374023\n",
      "1.0687620639801025\n",
      "1.0706795454025269\n",
      "1.0684003829956055\n",
      "1.0705064535140991\n",
      "1.0680396556854248\n",
      "1.0703341960906982\n",
      "1.06768000125885\n",
      "1.0701625347137451\n",
      "1.067321538925171\n",
      "1.0699917078018188\n",
      "1.0669641494750977\n",
      "1.0698214769363403\n",
      "1.06660795211792\n",
      "1.0696518421173096\n",
      "1.0662528276443481\n",
      "1.0694830417633057\n",
      "1.0658987760543823\n",
      "1.06931471824646\n",
      "1.0655457973480225\n",
      "1.0691473484039307\n",
      "1.0651941299438477\n",
      "1.0689804553985596\n",
      "1.0648432970046997\n",
      "1.0688143968582153\n",
      "1.0644936561584473\n",
      "1.0686486959457397\n",
      "1.0641450881958008\n",
      "1.068483829498291\n",
      "1.0637975931167603\n",
      "1.0683196783065796\n",
      "1.0634510517120361\n",
      "1.0681560039520264\n",
      "1.063105583190918\n",
      "1.067992925643921\n",
      "1.0627613067626953\n",
      "1.0678306818008423\n",
      "1.062417984008789\n",
      "1.0676689147949219\n",
      "1.0620756149291992\n",
      "1.0675078630447388\n",
      "1.0617343187332153\n",
      "1.0673472881317139\n",
      "1.061394214630127\n",
      "1.0671875476837158\n",
      "1.0610549449920654\n",
      "1.067028284072876\n",
      "1.0607167482376099\n",
      "1.0668694972991943\n",
      "1.0603795051574707\n",
      "1.066711664199829\n",
      "1.0600433349609375\n",
      "1.066554069519043\n",
      "1.0597081184387207\n",
      "1.0663973093032837\n",
      "1.0593738555908203\n",
      "1.0662410259246826\n",
      "1.0590407848358154\n",
      "1.0660853385925293\n",
      "1.0587084293365479\n",
      "1.0659301280975342\n",
      "1.0583771467208862\n",
      "1.0657756328582764\n",
      "1.058046817779541\n",
      "1.0656217336654663\n",
      "1.0577174425125122\n",
      "1.0654683113098145\n",
      "1.0573890209197998\n",
      "1.0653154850006104\n",
      "1.0570616722106934\n",
      "1.0651631355285645\n",
      "1.0567351579666138\n",
      "1.0650115013122559\n",
      "1.0564095973968506\n",
      "1.064860224723816\n",
      "1.0560848712921143\n",
      "1.0647096633911133\n",
      "1.0557610988616943\n",
      "1.0645595788955688\n",
      "1.05543851852417\n",
      "1.0644099712371826\n",
      "1.0551165342330933\n",
      "1.0642611980438232\n",
      "1.054795742034912\n",
      "1.0641125440597534\n",
      "1.0544755458831787\n",
      "1.063964605331421\n",
      "1.0541563034057617\n",
      "1.0638171434402466\n",
      "1.0538381338119507\n",
      "1.0636703968048096\n",
      "1.053520679473877\n",
      "1.0635238885879517\n",
      "1.0532042980194092\n",
      "1.0633779764175415\n",
      "1.0528886318206787\n",
      "1.063232660293579\n",
      "1.0525739192962646\n",
      "1.0630877017974854\n",
      "1.0522600412368774\n",
      "1.0629433393478394\n",
      "1.0519471168518066\n",
      "1.0627995729446411\n",
      "1.0516350269317627\n",
      "1.0626561641693115\n",
      "1.051323652267456\n",
      "1.0625131130218506\n",
      "1.0510132312774658\n",
      "1.062370777130127\n",
      "1.0507036447525024\n",
      "1.0622289180755615\n",
      "1.050394892692566\n",
      "1.0620874166488647\n",
      "1.0500869750976562\n",
      "1.0619465112686157\n",
      "1.0497798919677734\n",
      "1.0618059635162354\n",
      "1.0494736433029175\n",
      "1.0616657733917236\n",
      "1.0491682291030884\n",
      "1.0615262985229492\n",
      "1.0488635301589966\n",
      "1.061387300491333\n",
      "1.0485597848892212\n",
      "1.061248540878296\n",
      "1.048256754875183\n",
      "1.061110258102417\n",
      "1.0479545593261719\n",
      "1.0609724521636963\n",
      "1.047653079032898\n",
      "1.0608351230621338\n",
      "1.0473525524139404\n",
      "1.0606982707977295\n",
      "1.0470527410507202\n",
      "1.0605618953704834\n",
      "1.0467536449432373\n",
      "1.060425877571106\n",
      "1.0464553833007812\n",
      "1.0602902173995972\n",
      "1.0461578369140625\n",
      "1.0601551532745361\n",
      "1.0458611249923706\n",
      "1.0600204467773438\n",
      "1.045565128326416\n",
      "1.0598862171173096\n",
      "1.0452698469161987\n",
      "1.0597522258758545\n",
      "1.0449753999710083\n",
      "1.0596187114715576\n",
      "1.0446816682815552\n",
      "1.0594857931137085\n",
      "1.0443886518478394\n",
      "1.0593531131744385\n",
      "1.0440964698791504\n",
      "1.0592209100723267\n",
      "1.0438048839569092\n",
      "1.059088945388794\n",
      "1.0435141324996948\n",
      "1.058957576751709\n",
      "1.0432240962982178\n",
      "1.0588265657424927\n",
      "1.042934775352478\n",
      "1.0586957931518555\n",
      "1.0426461696624756\n",
      "1.0585657358169556\n",
      "1.0423582792282104\n",
      "1.0584356784820557\n",
      "1.0420711040496826\n",
      "1.058306336402893\n",
      "1.041784644126892\n",
      "1.05817711353302\n",
      "1.0414987802505493\n",
      "1.0580483675003052\n",
      "1.0412137508392334\n",
      "1.057919979095459\n",
      "1.0409293174743652\n",
      "1.0577919483184814\n",
      "1.040645718574524\n",
      "1.057664394378662\n",
      "1.0403625965118408\n",
      "1.0575371980667114\n",
      "1.040080189704895\n",
      "1.0574102401733398\n",
      "1.039798617362976\n",
      "1.057283639907837\n",
      "1.0395175218582153\n",
      "1.0571573972702026\n",
      "1.0392372608184814\n",
      "1.0570316314697266\n",
      "1.0389575958251953\n",
      "1.0569062232971191\n",
      "1.038678526878357\n",
      "1.0567810535430908\n",
      "1.0384001731872559\n",
      "1.0566562414169312\n",
      "1.038122296333313\n",
      "1.0565319061279297\n",
      "1.0378453731536865\n",
      "1.0564076900482178\n",
      "1.0375689268112183\n",
      "1.056283950805664\n",
      "1.0372930765151978\n",
      "1.0561604499816895\n",
      "1.037018060684204\n",
      "1.0560373067855835\n",
      "1.036743402481079\n",
      "1.0559146404266357\n",
      "1.036469578742981\n",
      "1.0557920932769775\n",
      "1.036196231842041\n",
      "1.0556700229644775\n",
      "1.0359234809875488\n",
      "1.055548071861267\n",
      "1.0356515645980835\n",
      "1.0554267168045044\n",
      "1.0353801250457764\n",
      "1.0553053617477417\n",
      "1.0351091623306274\n",
      "1.0551844835281372\n",
      "1.0348390340805054\n",
      "1.0550639629364014\n",
      "1.0345693826675415\n",
      "1.0549437999725342\n",
      "1.0343003273010254\n",
      "1.054823637008667\n",
      "1.034031867980957\n",
      "1.054703950881958\n",
      "1.0337640047073364\n",
      "1.0545846223831177\n",
      "1.0334967374801636\n",
      "1.0544655323028564\n",
      "1.0332300662994385\n",
      "1.0543467998504639\n",
      "1.0329638719558716\n",
      "1.05422842502594\n",
      "1.032698392868042\n",
      "1.0541101694107056\n",
      "1.032433271408081\n",
      "1.0539921522140503\n",
      "1.0321688652038574\n",
      "1.0538746118545532\n",
      "1.0319050550460815\n",
      "1.0537571907043457\n",
      "1.0316417217254639\n",
      "1.0536401271820068\n",
      "1.031378984451294\n",
      "1.053523302078247\n",
      "1.0311167240142822\n",
      "1.0534067153930664\n",
      "1.0308551788330078\n",
      "1.053290605545044\n",
      "1.030593991279602\n",
      "1.0531744956970215\n",
      "1.030333399772644\n",
      "1.0530588626861572\n",
      "1.0300734043121338\n",
      "1.052943229675293\n",
      "1.0298138856887817\n",
      "1.052828073501587\n",
      "1.029554843902588\n",
      "1.05271315574646\n",
      "1.0292965173721313\n",
      "1.0525983572006226\n",
      "1.0290385484695435\n",
      "1.0524839162826538\n",
      "1.0287811756134033\n",
      "1.0523697137832642\n",
      "1.0285241603851318\n",
      "1.0522557497024536\n",
      "1.0282678604125977\n",
      "1.0521420240402222\n",
      "1.0280120372772217\n",
      "1.0520285367965698\n",
      "1.0277565717697144\n",
      "1.0519154071807861\n",
      "1.0275017023086548\n",
      "1.0518022775650024\n",
      "1.0272473096847534\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,300): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        tag_scores = model(sentence_in)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:24.449994Z",
     "start_time": "2019-06-09T11:08:24.433038Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9594, -1.0523, -1.3177],\n",
      "        [-1.1054, -0.9684, -1.2404],\n",
      "        [-1.0120, -0.9800, -1.3425],\n",
      "        [-0.9985, -0.9675, -1.3801],\n",
      "        [-0.9888, -0.9677, -1.3941]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print (tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T11:08:26.155433Z",
     "start_time": "2019-06-09T11:08:26.139477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9594, -1.0523, -1.3177],\n",
      "        [-1.1054, -0.9684, -1.2404],\n",
      "        [-1.0120, -0.9800, -1.3425],\n",
      "        [-0.9985, -0.9675, -1.3801],\n",
      "        [-0.9888, -0.9677, -1.3941]])\n",
      "tensor([0, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_scores = tag_scores.detach().squeeze(-1)\n",
    "print(tag_scores)\n",
    "temp = np.argmax(tag_scores, axis=1)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-01T13:22:15.403893Z",
     "start_time": "2019-06-01T13:22:15.397909Z"
    }
   },
   "source": [
    "## 实现 LSTM + CRF 序列标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里之所以使用 CRF 来对 LSTM 的最后一层输出进行处理，是因为我们 LSTM 输出的数据具有较的随机性，而且序列标注的结果标签与标签之间的转移关系是有一定规律性的，也就需要加入新的条件满足输出对这种规律性的反应。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF 的参考资料可以参考柯林斯大学的[NLP概率模型讲解](http://www.cs.columbia.edu/~mcollins/)，这部分主要是针对 NLP 领域的一些概率分布的介绍，求解 CRF 过程中使用的 Vertibi 算法可以参考[知乎点赞最高的回复](https://www.zhihu.com/question/20136144)，答主给出的回复十分通俗便于理解，同时由于这部分的代码实现的复杂性，个人独立短时间内实现太过于困难，所以 CRF 代码我参考了 github 上对 CNN+BiLSTM+CRF 论文的还原代码，通过对 CONLL2003 数据进行预处理，完成对序列标注任务;同时在这部分我也了解了[判别式模型与生成式模型的区别以及HMM与MEMM的区别](https://www.zhihu.com/question/35866596)，但是由于时间学习的时间比较短，这些东西也只是看了个大概，具体深究还得等随后在暑假期间继续进行深究。\n",
    "\n",
    "（注：对于 CRF 的学习我想随后会进一步进行深究，因为这部分只是看懂了一个算法的大概，具体的细节部分还需要后续继续阅读相关资料补充图论与概率论的知识）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:09.834816Z",
     "start_time": "2019-06-10T02:26:09.822587Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_scalar(var):\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "# 实现argmax函数\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算单词与标注之间的关系：\n",
    "$$ P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:11.661527Z",
     "start_time": "2019-06-10T02:26:11.651857Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概率无向图的联合概率分布：  \n",
    "\n",
    "$$P(Y)=\\frac{1}{Z(x)} \\prod_{c} \\psi_{c}\\left(Y_{c}\\right)=\\frac{1}{Z(x)} \\prod_{c} e^{\\sum_{k} \\lambda_{k} f_{k}(c, y | c, x)}=\\frac{1}{Z(x)} e^{\\sum_{c} \\sum_{k} \\lambda_{k} f_{k}\\left(y_{i}, y_{i}, x, i\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:39.939097Z",
     "start_time": "2019-06-10T02:26:39.897013Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实现 BiLSTM + CRF类\n",
    "from torch.autograd import Variable \n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)  \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return ( Variable( torch.randn(2, 1, self.hidden_dim)),\n",
    "                 Variable( torch.randn(2, 1, self.hidden_dim)) )\n",
    "    \n",
    "    # 实现前项算法\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = Variable(init_alphas)\n",
    "        # 计算在标签传递过程中的历史值\n",
    "        for feat in feats:\n",
    "            alphas_t = [] \n",
    "            for next_tag in range(0,self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).unsqueeze(0))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    # 将序列通过 LSTM 处理之后的隐藏层特征值取出\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    # 计算句子在当前标注下的得分\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = Variable( torch.Tensor([0]) )\n",
    "        tags = torch.cat( [torch.LongTensor([self.tag_to_ix[START_TAG]]), tags] )\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]] + feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    # 实现 viterbi 算法\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = Variable(init_vvars)\n",
    "        for feat in feats:\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = [] \n",
    "            for next_tag in range(0,self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].unsqueeze(0))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        # 处理 STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "        # 找寻最佳路线\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] \n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "    \n",
    "    # 使用 log 级别的参数传递过程\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        self.hidden = self.init_hidden()\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "    # 应用于进行模型运算的前项传递算法\n",
    "    def forward(self, sentence): \n",
    "        self.hidden = self.init_hidden()\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提高标签的辨识准确度，对数据进行预处理时候，将所有的数字替换为 0 ，保证数字不对序列标注产生过大的影响；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:43.238259Z",
     "start_time": "2019-06-10T02:26:43.218093Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "# 对训练集数据进行预处理\n",
    "def zero_digits(s):\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "# 保证读入的每一行都至少有一个单词跟一个标签\n",
    "def load_sentences(path, zeros):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:50.111199Z",
     "start_time": "2019-06-10T02:26:47.252418Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对数据进行处理\n",
    "train_sentences = load_sentences('C:/Users/WYX/Desktop/NLP训练/Task4/conll-corpora/eng.train', True)\n",
    "test_sentences = load_sentences('C:/Users/WYX/Desktop/NLP训练/Task4/conll-corpora/eng.testa', True)\n",
    "dev_sentences = load_sentences('C:/Users/WYX/Desktop/NLP训练/Task4/conll-corpora/eng.testb', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:50.148023Z",
     "start_time": "2019-06-10T02:26:50.116426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'NNP', 'I-NP', 'I-ORG'], ['rejects', 'VBZ', 'I-VP', 'O'], ['German', 'JJ', 'I-NP', 'I-MISC'], ['call', 'NN', 'I-NP', 'O'], ['to', 'TO', 'I-VP', 'O'], ['boycott', 'VB', 'I-VP', 'O'], ['British', 'JJ', 'I-NP', 'I-MISC'], ['lamb', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:50.217619Z",
     "start_time": "2019-06-10T02:26:50.193745Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对标签与句子进行映射处理\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "def create_dico(item_list):\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 \n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    print(dico['the'])\n",
    "    \n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "# 拿出第二列用于做实体识别\n",
    "def tag_mapping(sentences):\n",
    "    tags = [[word[-3] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    print(dico)\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:52.671927Z",
     "start_time": "2019-06-10T02:26:52.235638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17493 unique words (203621 in total)\n",
      "8390\n",
      "Found 47 unique named entity tags\n",
      "{'NNP': 34392, 'VBZ': 2426, 'JJ': 11831, 'NN': 23899, 'TO': 3469, 'VB': 4252, '.': 7389, 'CD': 19704, 'DT': 13453, 'VBD': 8293, 'IN': 19064, 'PRP': 3163, 'NNS': 9903, 'VBP': 1436, 'MD': 1199, 'VBN': 4105, 'POS': 1553, 'JJR': 382, '\"': 2178, 'RB': 3975, ',': 7291, 'FW': 166, 'CC': 3653, 'WDT': 506, '(': 2866, ')': 2866, ':': 2386, 'PRP$': 1520, 'RBR': 163, 'VBG': 2585, 'EX': 136, 'WP': 528, 'WRB': 384, '$': 427, 'RP': 528, 'NNPS': 684, 'SYM': 439, 'RBS': 35, 'UH': 30, 'PDT': 33, \"''\": 35, 'LS': 13, 'JJS': 254, 'WP$': 23, 'NN|SYM': 4, '<START>': -1, '<STOP>': -2}\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, True)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:54.662536Z",
     "start_time": "2019-06-10T02:26:54.180151Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将上步完成映射的单词拼成可进行处理的句子\n",
    "# 保证所有单词均为小写状态\n",
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x\n",
    "def prepare_dataset(sentences, word_to_id, tag_to_id, lower=True):\n",
    "    \"\"\"\n",
    "   处理后保证每一个项含有：\n",
    "        - word indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-3]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, tag_to_id,True\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, tag_to_id, True\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, tag_to_id, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:26:56.768006Z",
     "start_time": "2019-06-10T02:26:56.756825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str_words': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'words': [944, 15473, 198, 590, 8, 3848, 207, 6233, 2], 'tags': [0, 19, 5, 1, 14, 10, 5, 1, 8]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:27:21.536610Z",
     "start_time": "2019-06-10T02:27:21.450484Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "import torch.optim as optim\n",
    "EMBEDDING_DIM = 200 \n",
    "HIDDEN_DIM = 50\n",
    "model = BiLSTM_CRF(len(word_to_id), tag_to_id, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T02:27:23.420823Z",
     "start_time": "2019-06-10T02:27:23.280432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944, 15473, 198, 590, 8, 3848, 207, 6233, 2]\n",
      "(tensor(23.7454, grad_fn=<SelectBackward>), [5, 29, 20, 37, 41, 42, 23, 28, 0])\n"
     ]
    }
   ],
   "source": [
    "# 测试模型是否能够调通\n",
    "precheck_sent = train_data[0]['words']\n",
    "print(precheck_sent)\n",
    "print (model(torch.tensor(precheck_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:25:50.932591Z",
     "start_time": "2019-06-10T03:24:48.518377Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "for epoch in range(0,1): \n",
    "    for train in train_data[0:100]:\n",
    "        model.zero_grad()\n",
    "        neg_log_likelihood = model.neg_log_likelihood(torch.tensor(train['words']),torch.tensor(train['tags']))\n",
    "        #print(neg_log_likelihood.item())\n",
    "        neg_log_likelihood.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:25:51.018186Z",
     "start_time": "2019-06-10T03:25:50.935583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 14, 0, 0, 0, 8]\n",
      "[0, 19, 5, 1, 14, 10, 5, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "_ , tag = model(torch.tensor(precheck_sent))\n",
    "print(tag)\n",
    "print(train_data[0]['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:36:37.659856Z",
     "start_time": "2019-06-10T03:36:26.008760Z"
    }
   },
   "outputs": [],
   "source": [
    "# 召回率、准确率、F1的计算\n",
    "total_now = []\n",
    "total_preds = []\n",
    "for i in train_data[0:100]:\n",
    "    total_now.extend(i['tags'])\n",
    "    #print(i['tags'])\n",
    "    _ , tag = model(torch.tensor(i['words']))\n",
    "    #print(tag)\n",
    "    total_preds.extend(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T03:39:01.099675Z",
     "start_time": "2019-06-10T03:39:00.810509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "The order of tags which is in dico 0\n",
      "0.25909878682842286\n",
      "0.964516129032258\n",
      "0.40846994535519127\n",
      "--------------\n",
      "The order of tags which is in dico 1\n",
      "0.4370860927152318\n",
      "0.3055555555555556\n",
      "0.3596730245231608\n",
      "--------------\n",
      "The order of tags which is in dico 2\n",
      "0.5\n",
      "0.023529411764705882\n",
      "0.0449438202247191\n",
      "--------------\n",
      "The order of tags which is in dico 3\n",
      "0.7083333333333334\n",
      "0.4358974358974359\n",
      "0.5396825396825398\n",
      "--------------\n",
      "The order of tags which is in dico 4\n",
      "0.8080808080808081\n",
      "0.7339449541284404\n",
      "0.7692307692307693\n",
      "--------------\n",
      "The order of tags which is in dico 5\n",
      "0.23529411764705882\n",
      "0.037037037037037035\n",
      "0.064\n",
      "--------------\n",
      "The order of tags which is in dico 6\n",
      "0.2857142857142857\n",
      "0.04040404040404041\n",
      "0.07079646017699116\n",
      "--------------\n",
      "The order of tags which is in dico 7\n",
      "1.0\n",
      "0.025\n",
      "0.04878048780487806\n",
      "--------------\n",
      "The order of tags which is in dico 8\n",
      "1.0\n",
      "0.967741935483871\n",
      "0.9836065573770492\n",
      "--------------\n",
      "The order of tags which is in dico 14\n",
      "0.9565217391304348\n",
      "1.0\n",
      "0.9777777777777777\n",
      "--------------\n",
      "The order of tags which is in dico 19\n",
      "1.0\n",
      "0.034482758620689655\n",
      "0.06666666666666667\n",
      "--------------\n",
      "The order of tags which is in dico 26\n",
      "1.0\n",
      "0.16666666666666666\n",
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "TP = 0 \n",
    "FP = 0 \n",
    "FN = 0\n",
    "#print((total_now))\n",
    "#print((total_preds))\n",
    "#print(len(total_now&total_preds))\n",
    "for i in range(len(dico_tags)):\n",
    "    TP = 0 \n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for j in range(len(total_now)):\n",
    "        if i == total_now[j] and i == total_preds[j] :\n",
    "            #print(i)\n",
    "            TP += 1\n",
    "        elif i == total_now[j] and i != total_preds[j]:\n",
    "            #print(i)\n",
    "            FN += 1\n",
    "        elif i == total_preds[j] and i != total_now[j]:\n",
    "            FP += 1\n",
    "    if TP + FP == 0 or TP + FN == 0 :\n",
    "        continue \n",
    "    P = TP/(TP+FP)\n",
    "    R = TP/(TP+FN)\n",
    "    if P + R == 0 :\n",
    "        continue\n",
    "    F1 = 2 * P * R / (P+R)\n",
    "    print('--------------')\n",
    "    print('The order of tags which is in dico',i)\n",
    "    print(P)\n",
    "    print(R)\n",
    "    print(F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
