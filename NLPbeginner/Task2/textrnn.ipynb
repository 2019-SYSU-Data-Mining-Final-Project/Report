{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + GRU 实现文本情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:31:40.963616Z",
     "start_time": "2019-06-10T17:31:35.314366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30359\n"
     ]
    }
   ],
   "source": [
    "# 读入数据\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('C:/Users/WYX/Desktop/test/data/train.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "test_data = pd.read_csv('C:/Users/WYX/Desktop/test/data/test.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "def textPreprocess(text):\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 预处理文本\n",
    "train_data['Sentence'] = train_data['Sentence'].apply(textPreprocess)\n",
    "test_data['Sentence'] = test_data['Sentence'].apply(textPreprocess)\n",
    "#print(train_data['Sentence'])\n",
    "num_train_texts = train_data['Sentence'].size\n",
    "num_test_texts = test_data['Sentence'].size\n",
    "# 收集 vocab\n",
    "word_set = set()\n",
    "for i in range(0, num_train_texts):\n",
    "    word_set.update([w.lower() for w in word_tokenizer.tokenize(train_data['Sentence'][i])])\n",
    "for i in range(0, num_test_texts):\n",
    "    word_set.update([w.lower() for w in word_tokenizer.tokenize(test_data['Sentence'][i])])\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:31:40.981547Z",
     "start_time": "2019-06-10T17:31:40.969575Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义 load_embeding 可以选择使用使用谷歌预训练的glove 或者 个人在textcnn中训练好的 embeding\n",
    "def load_glove_embedding(word_list, uniform_scale, dimension_size):\n",
    "    index = 0\n",
    "    if os.path.exists('glove_embedding.npy'):\n",
    "        word_vectors = np.load('glove_embedding.npy')\n",
    "        print('Successfully load saved embedding!')\n",
    "    else:\n",
    "        glove_words = {}\n",
    "        #with open('C:/Users/WYX/Desktop/test/embedding_word2vec.txt', 'r',encoding='utf-8') as fopen:\n",
    "        with open('C:/Users/WYX/Desktop/test/glove.840B.300d.txt', 'r',encoding='utf-8') as fopen:\n",
    "            for line in fopen:\n",
    "                tmp = line.split(' ')\n",
    "                glove_words[tmp[0]] = np.array(tmp[1:], dtype=np.float32)\n",
    "\n",
    "        word_to_index = {}\n",
    "        word_vectors = np.zeros([len(word_set), 300])\n",
    "        for word in word_list:\n",
    "            word_to_index[word] = index\n",
    "            index += 1\n",
    "            if word in glove_words:\n",
    "                word_vectors[word_to_index[word]] = (glove_words[word])\n",
    "            elif word == '<pad>':\n",
    "                word_vectors[word_to_index[word]] = (np.zeros(dimension_size, dtype=np.float32))\n",
    "            else:\n",
    "                word_vectors[word_to_index[word]] = (np.random.uniform(-uniform_scale, uniform_scale, dimension_size))\n",
    "        \n",
    "    return word_vectors, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:32:22.440857Z",
     "start_time": "2019-06-10T17:31:43.376125Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "word_vectors, word_to_index = load_glove_embedding(word_set, 0.25, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:32:22.824983Z",
     "start_time": "2019-06-10T17:32:22.440857Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置禁用 gpu\n",
    "import torch\n",
    "is_cuda = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:32:22.844938Z",
     "start_time": "2019-06-10T17:32:22.824983Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 RNN + GRU 类\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0, bidirectional=False):\n",
    "        super(BaseGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1   \n",
    "        self.embedding = nn.Embedding(30359, 300)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(word_vectors))\n",
    "        self.gru = nn.GRU(300, hidden_size, num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.h2o = nn.Linear(self.num_directions * hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)  \n",
    "    def forward(self, inputs):\n",
    "        hidden = self.initHidden(is_cuda)\n",
    "        line = self.embedding(inputs)\n",
    "        line = torch.transpose(line, 0, 1)\n",
    "        output, hidden = self.gru(line, hidden)\n",
    "        output = self.h2o(output[line.size(0)-1])\n",
    "        output = self.softmax(output)\n",
    "        return output    \n",
    "    def initHidden(self, is_cuda=True):\n",
    "        if is_cuda:\n",
    "            hidden = torch.zeros(self.num_layers*self.num_directions, 1, self.hidden_size).cuda()\n",
    "        else:\n",
    "            hidden = torch.zeros(self.num_layers*self.num_directions, 1, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:32:22.964903Z",
     "start_time": "2019-06-10T17:32:22.844938Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "num_of_training_set = int(0.9 * num_train_texts)\n",
    "#print(train_data)\n",
    "train_data['Sentiment'] = train_data['Sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "training_set = train_data[:num_of_training_set]\n",
    "validation_set = train_data[num_of_training_set:]\n",
    "#print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:51:46.897559Z",
     "start_time": "2019-06-10T17:51:46.888579Z"
    }
   },
   "outputs": [],
   "source": [
    "def lineToIndex(line):\n",
    "    words = [w.lower() for w in word_tokenizer.tokenize(line)]\n",
    "    words_index = []\n",
    "    for w in words:\n",
    "        words_index.append(word_to_index[w])\n",
    "    lineIndex = torch.tensor([words_index])\n",
    "    return lineIndex\n",
    "import random\n",
    "def randomChoice(pdFrame):\n",
    "    idx = random.randint(0, len(pdFrame) - 1)\n",
    "    text = pdFrame.iloc[idx]['Sentence']\n",
    "    polarity = pdFrame.iloc[idx]['Sentiment']\n",
    "    print(pdFrame.iloc[idx]['Sentiment'].size)\n",
    "    text_index = lineToIndex(text)\n",
    "    polarity_tensor = torch.tensor([polarity], dtype=torch.long)\n",
    "    return idx, text_index, polarity_tensor\n",
    "\n",
    "def polarityFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    polarity = top_i[0].item()\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:37:58.134871Z",
     "start_time": "2019-06-10T17:37:58.121894Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.05 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "\n",
    "def train(model, category_tensor, line_tensor, weight_clip=0.1):\n",
    "    output = model(line_tensor)\n",
    "    model.zero_grad()\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    for p in model.parameters():\n",
    "        if hasattr(p.grad, \"data\"):\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "    return output, loss.item()\n",
    "\n",
    "def evaluate(model, n_train_eval, n_test_eval, save_threshold):\n",
    "    global save_count\n",
    "    model.eval()\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    for i in range(n_train_eval):\n",
    "        idx, text_index, polarity_tensor = randomChoice(training_set)\n",
    "        output = model(text_index)\n",
    "        predict = polarityFromOutput(output)\n",
    "        if predict == polarity_tensor.item():\n",
    "            train_correct += 1\n",
    "    for i in range(n_test_eval):\n",
    "        idx, text_index, polarity_tensor = randomChoice(validation_set)\n",
    "        output = model(text_index)\n",
    "        predict = polarityFromOutput(output)\n",
    "        if predict == polarity_tensor.item():\n",
    "            val_correct += 1\n",
    "    print('train set acc: {} | val set acc {}'.format(train_correct/n_train_eval, val_correct/n_test_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T17:50:51.656750Z",
     "start_time": "2019-06-10T17:38:12.065027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set acc: 0.518 | val set acc 0.504\n",
      "3m 37s 1000/30000 loss: 0.9887931494340301\n",
      "train set acc: 0.56 | val set acc 0.566\n",
      "7m 2s 2000/30000 loss: 0.9400041382387281\n",
      "train set acc: 0.55 | val set acc 0.502\n",
      "10m 28s 3000/30000 loss: 0.9727532647494227\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-91b7109f1702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarity_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomChoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarity_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m#print(text_index.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-db4fc19af3fa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, category_tensor, line_tensor, weight_clip)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "learning_rate = 0.05\n",
    "n_hidden = 128\n",
    "model = BaseGRU(300, n_hidden, 3, 2, 0, bidirectional=True)\n",
    "\n",
    "n_iters = 30000\n",
    "print_every = 1000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "last_loss = 0\n",
    "all_losses = [0]\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    idx, text_index, polarity_tensor = randomChoice(training_set)\n",
    "    _, loss = train(model, polarity_tensor, text_index)\n",
    "    #print(text_index.shape)\n",
    "    current_loss += loss\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        evaluate(model, n_train_eval=500, n_test_eval=500, save_threshold=0.72)\n",
    "        model.train()\n",
    "    if iter % plot_every == 0:\n",
    "        if all_losses[-1] < (current_loss / plot_every):\n",
    "            learning_rate *= 0.9\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        print('{} {}/{} loss: {}'.format(timeSince(start), iter, n_iters, current_loss / plot_every))\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于个人机器能力有限 在保证 rnn 运行正确的基础上停止了长时间运行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
