{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytroch 学习过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:17:41.930234Z",
     "start_time": "2019-05-15T19:17:41.926245Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:17:43.686865Z",
     "start_time": "2019-05-15T19:17:43.653070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.9694e-39, 7.2551e-39, 6.2449e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 8.4490e-39],\n",
      "        [9.6429e-39, 9.2755e-39, 1.0286e-38],\n",
      "        [9.0919e-39, 8.9082e-39, 9.2755e-39],\n",
      "        [8.4490e-39, 1.0194e-38, 9.0919e-39]])\n",
      "tensor([[0.9984, 0.8208, 0.1584],\n",
      "        [0.0465, 0.5781, 0.0802],\n",
      "        [0.7116, 0.2185, 0.1064],\n",
      "        [0.5044, 0.1026, 0.6697],\n",
      "        [0.2475, 0.1576, 0.1387]])\n"
     ]
    }
   ],
   "source": [
    "# 不加初始化构造一个 5 * 3的矩阵\n",
    "x = torch.empty(5,3)\n",
    "print(x)\n",
    "# 初始化构造 5 * 3矩阵\n",
    "y = torch.rand(5,3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：`torch.Tensor` 是一种包含单一数据类型元素的多维矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:17:45.927908Z",
     "start_time": "2019-05-15T19:17:45.916875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# Construct a tensor directly from data:\n",
    "x = torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:08:03.430047Z",
     "start_time": "2019-05-13T03:08:03.416200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.0723,  0.0453, -0.6463],\n",
      "        [-0.6740, -1.0280,  0.3183],\n",
      "        [-0.2123, -0.6022, -1.6899],\n",
      "        [ 0.2951,  0.0717,  0.3614],\n",
      "        [ 0.9633, -0.8296, -0.3745]])\n"
     ]
    }
   ],
   "source": [
    "# create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:08:20.348502Z",
     "start_time": "2019-05-13T03:08:20.340519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Get its size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:10:12.792426Z",
     "start_time": "2019-05-13T03:10:12.776009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5473,  0.4169,  0.0296],\n",
      "        [-0.6419, -0.0466,  1.3012],\n",
      "        [ 0.1793,  0.1926, -1.5360],\n",
      "        [ 0.5820,  0.9546,  1.0707],\n",
      "        [ 1.4848, -0.7317,  0.4464]])\n",
      "tensor([[ 0.5473,  0.4169,  0.0296],\n",
      "        [-0.6419, -0.0466,  1.3012],\n",
      "        [ 0.1793,  0.1926, -1.5360],\n",
      "        [ 0.5820,  0.9546,  1.0707],\n",
      "        [ 1.4848, -0.7317,  0.4464]])\n",
      "tensor([[ 0.5473,  0.4169,  0.0296],\n",
      "        [-0.6419, -0.0466,  1.3012],\n",
      "        [ 0.1793,  0.1926, -1.5360],\n",
      "        [ 0.5820,  0.9546,  1.0707],\n",
      "        [ 1.4848, -0.7317,  0.4464]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(x+y)\n",
    "print(torch.add(x,y))\n",
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:12:34.321547Z",
     "start_time": "2019-05-13T03:12:34.314287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6195,  0.4621, -0.6166],\n",
      "        [-1.3159, -1.0746,  1.6195],\n",
      "        [-0.0330, -0.4096, -3.2260],\n",
      "        [ 0.8770,  1.0262,  1.4320],\n",
      "        [ 2.4481, -1.5613,  0.0719]])\n"
     ]
    }
   ],
   "source": [
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:14:26.507217Z",
     "start_time": "2019-05-13T03:14:26.494481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0453, -1.0280, -0.6022,  0.0717, -0.8296])\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:15:34.349130Z",
     "start_time": "2019-05-13T03:15:34.337806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Converting a Torch Tensor to a NumPy Array\n",
    "a = torch.ones(5);\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:15:49.446386Z",
     "start_time": "2019-05-13T03:15:49.430147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:17:50.686475Z",
     "start_time": "2019-05-15T19:17:50.670163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Converting NumPy Array to Torch Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a,1,out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T03:19:15.841668Z",
     "start_time": "2019-05-13T03:19:15.829029Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度学习的pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as True, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "\n",
    "To prevent tracking history (and using memory), you can also wrap the code block in with `torch.no_grad()`:. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a Function.\n",
    "\n",
    "Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their grad_fn is None).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:53:59.074109Z",
     "start_time": "2019-05-13T13:53:56.568038Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:53:59.231548Z",
     "start_time": "2019-05-13T13:53:59.078985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:53:59.254486Z",
     "start_time": "2019-05-13T13:53:59.235537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y was created as a result of an operation, so it has a `grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:54:01.111536Z",
     "start_time": "2019-05-13T13:54:01.090646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3 \n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:54:02.634563Z",
     "start_time": "2019-05-13T13:54:02.560714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000002BAA9B946D8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:54:04.265124Z",
     "start_time": "2019-05-13T13:54:04.216223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:59:19.456846Z",
     "start_time": "2019-05-13T13:59:19.355832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0176, -0.4099,  0.3306])\n",
      "tensor(2.0852)\n",
      "tensor([-4.0353, -0.8197,  0.6612])\n",
      "tensor(4.1704)\n",
      "tensor([-8.0705, -1.6395,  1.3224])\n",
      "tensor(8.3408)\n",
      "tensor([-16.1410,  -3.2789,   2.6448])\n",
      "tensor(16.6817)\n",
      "tensor([-32.2820,  -6.5579,   5.2896])\n",
      "tensor(33.3634)\n",
      "tensor([-64.5641, -13.1158,  10.5791])\n",
      "tensor(66.7268)\n",
      "tensor([-129.1282,  -26.2315,   21.1582])\n",
      "tensor(133.4535)\n",
      "tensor([-258.2563,  -52.4630,   42.3165])\n",
      "tensor(266.9071)\n",
      "tensor([-516.5126, -104.9261,   84.6330])\n",
      "tensor(533.8141)\n",
      "tensor([-1033.0253,  -209.8521,   169.2659], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x * 2 ;\n",
    "while y.data.norm() < 1000:\n",
    "    print(y.data)\n",
    "    print(y.data.norm())\n",
    "    y = y * 2;\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T13:59:23.412274Z",
     "start_time": "2019-05-13T13:59:23.401301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:00:42.566885Z",
     "start_time": "2019-05-13T14:00:42.546404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "print(x.grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T07:36:52.986416Z",
     "start_time": "2019-05-15T07:36:52.981445Z"
    }
   },
   "source": [
    "### word embeding 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embedding is an approach to provide a dense vector representation of words that capture something about their meaning.\n",
    "\n",
    "Word embeddings are an improvement over simpler bag-of-word model word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words.\n",
    "\n",
    "Word embeddings work by using an algorithm to train a set of fixed-length dense and continuous-valued vectors based on a large corpus of text. Each word is represented by a point in the embedding space and these points are learned and moved around based on the words that surround the target word.\n",
    "\n",
    "It is defining a word by the company that it keeps that allows the word embedding to learn something about the meaning of words. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space.\n",
    "\n",
    "The use of word embeddings over other text representations is one of the key methods that has led to breakthrough performance with deep neural networks on problems like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注**:  \n",
    "`word embeding` 其实是词袋模型的改进，在 task 1 中选择使用的`词袋模型`建立的向量中的 0 可能会占用过多的空间，导致我们浪费太多的无用空间在向量的保存上面，同时词袋模型也不能很好反应`词与词之间的相似性关系`，而且词袋模型很容易`隔断词与词之间的相互联系`；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于Gensim库构建 `word embeding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 word2vec embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is one algorithm for learning a word embedding from a text corpus.\n",
    "\n",
    "There are two main training algorithms that can be used to learn the embedding from text; they are continuous bag of words (CBOW) and skip grams.\n",
    "\n",
    "Gensim provides the Word2Vec class for working with a Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Learning a word embedding` from text involves loading and organizing the text into sentences and providing them to the constructor of a new Word2Vec() instance. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:38:36.675862Z",
     "start_time": "2019-05-16T07:38:35.518181Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "D:\\ANACONDA\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = \"like play basketball dance rap music\"\n",
    "model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "- size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "- window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "- min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "- workers: (default 3) The number of threads to use while training.\n",
    "sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面所给出的这些参数，统统是应用于word2vec函数中的，规定创建 word embeding 过程中的一些特殊需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T07:56:27.877710Z",
     "start_time": "2019-05-15T07:56:27.870729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'a']\n"
     ]
    }
   ],
   "source": [
    "# 打印学习好的词汇 token\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T08:02:03.606997Z",
     "start_time": "2019-05-15T08:02:03.562114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[-1.4843451e-03  1.7421880e-03 -3.6965567e-04  5.4466451e-04\n",
      "  4.9114134e-03 -4.1461815e-03  1.5052444e-04  3.5633883e-03\n",
      "  2.4346111e-04  1.1773163e-03 -2.4946758e-03 -2.9253820e-03\n",
      "  8.3326653e-04 -2.1874896e-05 -4.8536160e-03  2.3529611e-03\n",
      " -1.0252636e-03 -5.6460599e-04 -3.8007323e-03  2.0722076e-03\n",
      "  2.8510750e-03 -2.1302493e-03  2.4644409e-03 -3.4067023e-04\n",
      " -7.8525330e-04  4.7079628e-04  2.7827937e-03  3.6107708e-04\n",
      "  3.3716476e-03  1.5742212e-03  3.7341474e-03 -4.1354182e-03\n",
      "  1.6504226e-03 -1.0222066e-04 -3.8867055e-03  1.0258600e-03\n",
      "  2.4221262e-03 -7.0633326e-04  3.9879270e-03 -2.3659850e-03\n",
      "  3.2991129e-03  2.0806962e-03 -2.6877198e-04  3.5716174e-03\n",
      "  2.0463958e-03  3.6212353e-03  7.0278713e-04  1.9959810e-03\n",
      "  3.7956703e-03 -6.5343286e-04  9.1706769e-04  2.4014800e-03\n",
      " -1.6702205e-03  6.2492833e-04 -3.6832022e-03  4.8303739e-03\n",
      " -1.1321022e-03  2.4463595e-03  4.8777065e-03  3.9338577e-03\n",
      "  3.8661798e-03  2.0788324e-03 -4.6824170e-03 -2.5355450e-03\n",
      "  2.5421733e-03  4.0685819e-03 -1.8060765e-03  3.8061184e-03\n",
      "  2.0592883e-05  3.8691752e-03  1.9838635e-03  3.8722232e-03\n",
      "  1.9247773e-03  4.1291737e-03  2.6802204e-03  4.7306046e-03\n",
      " -3.7091295e-03  2.6313881e-03  1.7163105e-03 -2.6228447e-03\n",
      " -9.9769211e-04  4.0939869e-04  4.7384645e-03 -1.4223665e-03\n",
      " -3.8064765e-03  7.5161050e-04 -2.7919419e-03  4.9898159e-03\n",
      "  1.8827434e-03 -1.8228235e-03  3.3723519e-03  4.9497117e-03\n",
      "  3.9005745e-03 -4.4463770e-03 -3.5623380e-03 -4.5958655e-03\n",
      "  1.6213440e-03 -3.1273235e-03 -2.2477594e-03 -1.8707440e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# 定义数据\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']]\n",
    "# 训练模型\n",
    "model = Word2Vec(sentences,min_count=1)\n",
    "# 汇总加载的模型\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# 访问向量\n",
    "print(model['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you learn word embedding for your text data, it can be nice to explore it with visualization.\n",
    "\n",
    "You can use classical projection methods to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph.\n",
    "\n",
    "The visualizations can provide a qualitative diagnostic for your learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T08:26:54.067901Z",
     "start_time": "2019-05-15T08:26:54.062539Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a projection method on the vectors, such as those methods offered in scikit-learn, then use matplotlib to plot the projection as a scatter plot.\n",
    "\n",
    "Let’s look at an example with `Principal Component Analysis`（主成分分析） or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:50:14.466930Z",
     "start_time": "2019-05-15T19:50:12.813194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import  Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "# 定义语句\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']]\n",
    "# 训练模型\n",
    "model = Word2Vec(sentences,min_count=1)\n",
    "# 使用一个二维PCA模型去适应embeding vector\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# 创建散点图\n",
    "pyplot.scatter(result[:,0],result[:,1])\n",
    "words = list(model.wv.vocab)\n",
    "for i , word in enumerate(words):\n",
    "    pyplot.annotate(word,xy=(result[i,0],result[i,1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:00:40.652145Z",
     "start_time": "2019-05-15T09:00:40.589922Z"
    }
   },
   "source": [
    "### 装载谷歌 word2vec embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:50:06.287712Z",
     "start_time": "2019-05-15T19:50:04.743757Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYX\\AppData\\Roaming\\Python\\Python37\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c33774fcc22f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "# 预使用谷歌 word2vec embeding\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename,binary=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting thing that you can do is do a little linear algebra arithmetic with words.\n",
    "\n",
    "For example, a popular example described in lectures and introduction papers is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:21:55.986162Z",
     "start_time": "2019-05-15T09:18:58.527Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queen = (king - man) + woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the word queen is the closest word given the subtraction of the notion of man from king and adding the word woman. The “man-ness” in king is replaced with “woman-ness” to give us queen. A very cool concept.\n",
    "\n",
    "Gensim provides an interface for performing these types of operations in the most_similar() function on the trained or loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:21:55.990761Z",
     "start_time": "2019-05-15T09:19:00.287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-15T19:50:23.433Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# 装载模型\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "# 计算最接近\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:21:55.991413Z",
     "start_time": "2019-05-15T09:21:28.940Z"
    }
   },
   "source": [
    "### 装载 Stanford glove embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-15T19:50:32.584Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# 装入给定glove\n",
    "glove_input_file = 'C:/Users/WYX/Desktop/GCAE/model_files/glove.840B.300d.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "glove2word2vec(glove_input_file,word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-15T19:50:39.585Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename,binary=False)\n",
    "result = model.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:23:49.611348Z",
     "start_time": "2019-05-15T11:23:49.607356Z"
    }
   },
   "source": [
    "### 创建临时训练的 embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:05.560184Z",
     "start_time": "2019-05-16T07:16:05.394041Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive “1” or negative “0”. This is a simple sentiment analysis problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:07.505701Z",
     "start_time": "2019-05-16T07:16:07.497575Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:09.757700Z",
     "start_time": "2019-05-16T07:16:09.747238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44, 43], [44, 29], [1, 47], [8, 29], [15], [1], [45, 47], [18, 44], [45, 29], [39, 49, 43, 46]]\n"
     ]
    }
   ],
   "source": [
    "# 把整数编码成文档\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d,vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:34:18.113766Z",
     "start_time": "2019-05-15T11:34:18.106784Z"
    }
   },
   "source": [
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4. Again, we can do this with a built in Keras function, in this case the pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:11.729834Z",
     "start_time": "2019-05-16T07:16:11.717904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44 43  0  0]\n",
      " [44 29  0  0]\n",
      " [ 1 47  0  0]\n",
      " [ 8 29  0  0]\n",
      " [15  0  0  0]\n",
      " [ 1  0  0  0]\n",
      " [45 47  0  0]\n",
      " [18 44  0  0]\n",
      " [45 29  0  0]\n",
      " [39 49 43 46]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4 \n",
    "padded_docs = pad_sequences(encoded_docs,maxlen=max_length,padding= 'post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "\n",
    "The model is a simple binary classification model. Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:18.206979Z",
     "start_time": "2019-05-16T07:16:17.968606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,8,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:16:39.059284Z",
     "start_time": "2019-05-16T07:16:38.937191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs,labels,epochs=50,verbose=0)\n",
    "loss,accuracy = model.evaluate(padded_docs,labels,verbose=0)\n",
    "print('Accuracy: %f'%(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-15T09:39:23.524Z"
    }
   },
   "source": [
    "### 使用 Stanford 预训练glove embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word_index attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:21:41.858754Z",
     "start_time": "2019-05-16T07:21:41.849432Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:33:17.331681Z",
     "start_time": "2019-05-16T07:33:17.309167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "    'Good work',\n",
    "    'Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!',\n",
    "    'Weak',\n",
    "    'Poor effort!',\n",
    "    'not good',\n",
    "    'poor work',\n",
    "    'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:21:20.654897Z",
     "start_time": "2019-05-16T07:21:20.630297Z"
    }
   },
   "outputs": [],
   "source": [
    "embedings_index = dict()\n",
    "f = open('C:/Users/WYX/Desktop/GCAE/model_files/glove.840B.300d.txt','rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:29:26.241977Z",
     "start_time": "2019-05-16T07:21:44.697035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2196015 word vectors\n"
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:],dtype='float32')\n",
    "    embedings_index[word] = coefs \n",
    "f.close()\n",
    "print('Loaded %s word vectors'% len(embedings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:34:23.056313Z",
     "start_time": "2019-05-16T07:34:23.045915Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建权值矩阵\n",
    "embedding_matrix = zeros((vocab_size,100))\n",
    "for word ,i in t.word_index.items():\n",
    "    embedding_vector = embedings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:35:52.679403Z",
     "start_time": "2019-05-16T07:35:52.504022Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:36:02.181785Z",
     "start_time": "2019-05-16T07:36:02.028649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 编译\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:36:41.356695Z",
     "start_time": "2019-05-16T07:36:41.163884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.000000\n"
     ]
    }
   ],
   "source": [
    "# 适应模型\n",
    "model.fit(padded_docs, labels, epochs=100, verbose=0)\n",
    "# 评估\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 wordembeding 的词语预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:54:10.574034Z",
     "start_time": "2019-05-15T19:54:10.466680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7518,  0.5892, -0.3038, -0.3653, -1.1739]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "word_to_ix = {'hello': 0, 'world': 1}\n",
    "embeds = nn.Embedding(2, 5)\n",
    "hello_idx = torch.LongTensor([word_to_ix['hello']])\n",
    "hello_idx = Variable(hello_idx)\n",
    "hello_embed = embeds(hello_idx)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一篇文章中，每一句话可以由很多个单词组成，而对于一句话而言，这些单词的组成顺序也很重要；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N - gram 模型计算公式：\n",
    "$P(w_i|w_{i-1},w_{i-2},....,w_{i-n+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述公式可以看出，N - gram 公式是一个计算条件概率的公式，也即给定前面几个单词，最大化我们想要预测的单词的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:54:12.371643Z",
     "start_time": "2019-05-15T19:54:12.364662Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理 \n",
    "# 设置通过 2 个单词进行预测\n",
    "CONTEXT_SIZE = 2\n",
    "# 设置 word_embeding 的维数\n",
    "EMBEDDING_DIM = 10\n",
    "# 设置给定语料\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# 使用前两个为一个元组  后一个为待预测数据进行保存\n",
    "trigram = [((test_sentence[i],test_sentence[i+1]),test_sentence[i+2]) \n",
    "           for i in range(len(test_sentence)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:54:15.402450Z",
     "start_time": "2019-05-15T19:54:15.396453Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用 set 处理掉中间重复出现的词语 \n",
    "vocb = set(test_sentence)\n",
    "# 将数据整理好，也就是我们需要将单词三个分组，每个组前两个作为传入的数据，而最后一个作为预测的结果\n",
    "word_to_idx = {word:i for i ,word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]:word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T19:54:16.629141Z",
     "start_time": "2019-05-15T19:54:16.604931Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch.optim as optim\n",
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size*n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        print((emb.shape))\n",
    "        #print(emb)\n",
    "        #print(emb.view(1,-1))\n",
    "        emb = emb.view(1, -1)\n",
    "        print((emb.shape))\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_prob = F.log_softmax(out)\n",
    "        return log_prob\n",
    "\n",
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-14T17:03:49.879Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "for epoch in range(100):\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('*'*10)\n",
    "    running_loss = 0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        #print(type(word),word)\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        #print(type(label),label)\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        #print(out)\n",
    "        loss = criterion(out, label)\n",
    "        # 拿到 tensor 中的python number\n",
    "        running_loss += loss.item()\n",
    "        # 反向传播的过程只需要调用loss.backgrad()函数即可．但是由于变量的梯度是累加的，所以在求backward之前应该先对现有的梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 调用 step 更新梯度\n",
    "        optimizer.step()\n",
    "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T16:57:01.650801Z",
     "start_time": "2019-05-14T16:57:01.623873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -9.9988,  -8.0061,  -8.7443, -11.3829,  -9.6568,  -8.8099,  -9.7145,\n",
      "          -9.6482,  -8.4003,  -8.1090,  -9.3973,  -9.3107,  -8.1866,  -9.6622,\n",
      "          -8.0545, -10.0000,  -8.3407,  -8.4119,  -8.5360,  -8.5949,  -7.3294,\n",
      "          -8.5396,  -8.5868,  -9.6320,  -8.6006,  -8.0478,  -8.9333,  -8.8886,\n",
      "          -8.2759,  -8.8655,  -8.2310,  -8.1351,  -8.5338, -10.6826,  -6.5868,\n",
      "          -7.3014,  -8.4470,  -8.0026,  -7.9392,  -6.8618,  -9.1761,  -7.2728,\n",
      "          -6.7953,  -8.9893,  -8.3649,  -8.3360,  -8.8731,  -9.4642,  -9.1960,\n",
      "          -9.2868, -10.2940,  -5.6262,  -9.4397, -10.5414,  -9.0552, -10.1858,\n",
      "          -9.4227,  -8.6907,  -8.4254, -10.1058, -10.2626,  -9.0106, -10.9733,\n",
      "          -9.7702,  -0.0251,  -8.9284,  -7.9546,  -7.8535,  -9.6066,  -8.8238,\n",
      "          -9.0893,  -8.2352,  -8.0247,  -9.6315,  -8.5015,  -9.4875,  -8.8605,\n",
      "          -6.1824,  -9.1412,  -9.2430, -10.7242,  -9.4411,  -7.5835, -10.5182,\n",
      "          -8.9633,  -8.5399,  -9.3835,  -8.1014,  -8.7564,  -8.8937,  -8.2908,\n",
      "          -9.7650,  -9.1428,  -9.6160,  -9.1433,  -9.1302,  -9.1444]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "real word is now,, predict word is now,\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "word, label = trigram[20]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngrammodel(word)\n",
    "print(out)\n",
    "_, predict_label = torch.max(out.data, 1)\n",
    "#print(type(predict_label))\n",
    "predict_word = idx_to_word[predict_label.item()]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
